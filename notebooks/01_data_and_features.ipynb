{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-colab"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apropos0/Scheduling_Inference/blob/main/notebooks/01_data_and_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# 01 â€” Data + Features\n",
        "\n",
        "Goal:\n",
        "- Load one or more experiment CSVs\n",
        "- Confirm schema and balance across policy/workload/session\n",
        "- Compute basic time-normalized features\n",
        "- Save a clean artifact for downstream modeling\n",
        "\n",
        "Expected input columns:\n",
        "- timestamp, session_id, policy, workload\n",
        "- task_clock, context_switches, cpu_migrations\n",
        "- cycles, instructions, branches, branch_misses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 120)\n",
        "print(\"pandas\", pd.__version__)\n",
        "print(\"numpy\", np.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load-notes"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Set `DATA_PATHS` to one or more CSV files.\n",
        "\n",
        "Common options:\n",
        "- Upload CSV(s) into Colab runtime (left sidebar -> Files -> Upload), then use the uploaded filenames.\n",
        "- Keep CSV(s) in the repo under `data/` and reference them as `data/<name>.csv`.\n",
        "\n",
        "Tip: start with one session CSV first, then add more later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load-data"
      },
      "source": [
        "# Examples:\n",
        "# DATA_PATHS = [\"data/results_2025-12-31_A.csv\"]\n",
        "# DATA_PATHS = [\"results.csv\"]  # if uploaded into Colab\n",
        "\n",
        "DATA_PATHS = []\n",
        "\n",
        "if not DATA_PATHS:\n",
        "    raise ValueError(\"Set DATA_PATHS to one or more CSV filenames.\")\n",
        "\n",
        "dfs = []\n",
        "for p in DATA_PATHS:\n",
        "    if not Path(p).exists():\n",
        "        raise FileNotFoundError(f\"File not found: {p}\")\n",
        "    dfs.append(pd.read_csv(p))\n",
        "\n",
        "raw = pd.concat(dfs, ignore_index=True)\n",
        "print(\"Loaded shape:\", raw.shape)\n",
        "raw.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "schema-check"
      },
      "source": [
        "expected_cols = [\n",
        "    \"timestamp\",\"session_id\",\"policy\",\"workload\",\n",
        "    \"task_clock\",\"context_switches\",\"cpu_migrations\",\n",
        "    \"cycles\",\"instructions\",\"branches\",\"branch_misses\"\n",
        "]\n",
        "\n",
        "missing = [c for c in expected_cols if c not in raw.columns]\n",
        "extra = [c for c in raw.columns if c not in expected_cols]\n",
        "\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
        "print(\"Extra columns:\", extra)\n",
        "\n",
        "print(\"dtypes:\\n\", raw.dtypes)\n",
        "print(\"\\nMissing values per column:\\n\", raw.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "balance-checks"
      },
      "source": [
        "print(\"Policies:\\n\", raw[\"policy\"].value_counts(), \"\\n\")\n",
        "print(\"Workloads:\\n\", raw[\"workload\"].value_counts(), \"\\n\")\n",
        "print(\"Sessions:\\n\", raw[\"session_id\"].value_counts(), \"\\n\")\n",
        "\n",
        "print(\"Policy x Workload counts:\\n\")\n",
        "pd.crosstab(raw[\"policy\"], raw[\"workload\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sanity"
      },
      "source": [
        "## Sanity checks\n",
        "\n",
        "This is just an outlier scan to catch:\n",
        "- parser errors\n",
        "- missing values\n",
        "- weirdly small / huge numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sanity-stats"
      },
      "source": [
        "num_cols = [\n",
        "    \"task_clock\",\"context_switches\",\"cpu_migrations\",\n",
        "    \"cycles\",\"instructions\",\"branches\",\"branch_misses\"\n",
        "]\n",
        "raw[num_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "features"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "Compute:\n",
        "- time-normalized rates (per second)\n",
        "- ratios like IPC and miss rate\n",
        "\n",
        "These will be used in the modeling notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compute-features"
      },
      "source": [
        "df = raw.copy()\n",
        "\n",
        "# perf task-clock is in milliseconds here\n",
        "df[\"task_sec\"] = df[\"task_clock\"] / 1000.0\n",
        "\n",
        "# rates\n",
        "df[\"cs_per_sec\"] = df[\"context_switches\"] / df[\"task_sec\"]\n",
        "df[\"mig_per_sec\"] = df[\"cpu_migrations\"] / df[\"task_sec\"]\n",
        "df[\"cycles_per_sec\"] = df[\"cycles\"] / df[\"task_sec\"]\n",
        "df[\"instr_per_sec\"] = df[\"instructions\"] / df[\"task_sec\"]\n",
        "df[\"branches_per_sec\"] = df[\"branches\"] / df[\"task_sec\"]\n",
        "\n",
        "# ratios\n",
        "df[\"ipc\"] = df[\"instructions\"] / df[\"cycles\"]\n",
        "df[\"branch_miss_rate\"] = df[\"branch_misses\"] / df[\"branches\"]\n",
        "\n",
        "# safety\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "df[[\"cs_per_sec\",\"mig_per_sec\",\"ipc\",\"branch_miss_rate\"]].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nan-scan"
      },
      "source": [
        "df.isna().sum().sort_values(ascending=False).head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save"
      },
      "source": [
        "## Save clean dataset\n",
        "\n",
        "We save Parquet for speed + preserved dtypes.\n",
        "This is what Notebook 02 will load.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save-parquet"
      },
      "source": [
        "out_path = \"clean_results.parquet\"\n",
        "df.to_parquet(out_path, index=False)\n",
        "print(\"Wrote:\", out_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
